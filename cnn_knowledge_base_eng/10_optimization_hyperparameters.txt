OPTIMIZATION AND HYPERPARAMETERS
=================================

Q1: What are the main CNN hyperparameters?
A: Learning rate, batch size, optimizer, number of layers, number of filters, kernel size, dropout rate, weight decay.

Q2: Which optimizers to use?
A:
import torch.optim as optim

# SGD with momentum
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)

# Adam (adaptive)
optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))

# AdamW (Adam with corrected weight decay)
optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)

Q3: How to use learning rate schedulers?
A:
# StepLR: reduce LR every N epochs
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)

# MultiStepLR: reduce at specific epochs
scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 80], gamma=0.1)

# CosineAnnealingLR: cosine variation
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)

# ReduceLROnPlateau: reduce if validation plateaus
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=10)

# OneCycleLR: cyclic variation (very effective!)
scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, 
                                         steps_per_epoch=len(train_loader), 
                                         epochs=100)

Q4: How to do gradient clipping?
A:
optimizer.zero_grad()
loss.backward()

# Clip by norm
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# Clip by value
torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=0.5)

optimizer.step()

Q5: How to use mixed precision training?
A:
from torch.cuda.amp import autocast, GradScaler

model = MyModel().cuda()
optimizer = optim.Adam(model.parameters())
scaler = GradScaler()

for images, labels in train_loader:
    images, labels = images.cuda(), labels.cuda()
    
    optimizer.zero_grad()
    
    # Forward in mixed precision
    with autocast():
        outputs = model(images)
        loss = criterion(outputs, labels)
    
    # Backward with scaling
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
