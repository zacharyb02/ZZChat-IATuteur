R ´ESEAUX CONVOLUTIFS
1-
Introduction
2
1.1- Inspiration biologique
1.2- Convolution discr`ete
2-
D´eﬁnition des couches
3
2.1- Couche de convolution
2.2- Couche non lin´eaire
2.3- Couches de normalisation
2.4- Couche d’agr´egation et de sous-´echantillonnage
2.5- Couche compl`etement connect´ee
3-
R´egularisation
9
3.1- R´egularisation de la fonction de coˆut
3.2- Dropout
3.3- Partage de param`etres
4-
Initialisation
10
4.1- Initialisation prenant en compte les variations des neurones
4.2- Initialisation de He
5-
Apprentissage
11
5.1- Le probl`eme de l’entraˆınement
5.2- Apprentissage glouton par couche
5.3- Visualisation du m´ecanisme des r´eseaux convolutifs
5.4- M´ethodes de visualisation de base
5.5- M´ethodes fond´ees sur les activations
5.6- M´ethodes fond´ees sur le gradient
6-
Quelques applications
16
7-
Partie pratique
17
1-
INTRODUCTION
1.1-
Inspiration biologique
Un r´eseau de neurones convolutif (CNN, Convolutional Neural Network ou ConvNet) est un type de r´eseau
de neurones artiﬁciels acyclique `a propagation avant, dans lequel le motif de connexion entre les neurones
est inspir´e par le cortex visuel des animaux. Les neurones de cette r´egion du cerveau sont arrang´es de
sorte `a ce qu’ils correspondent `a des r´egions (appel´es champs r´eceptifs) qui se chevauchent lors du pavage
du champ visuel. Ils sont de plus organis´es de mani`ere hi´erarchique, en couches (aire visuelle primaire V1,
secondaire V2, puis aires V3, V4, V5 et V6, gyrus temporal inf´erieur), chacune des couches ´etant sp´ecialis´ee
dans une tˆache, de plus en plus abstraite en allant de l’entr´ee vers la sortie. En simpliﬁant `a l’extrˆeme, une
fois que les signaux lumineux sont rec¸us par la r´etine et convertis en potentiels d’action :
— L’aire primaire V1 s’int´eresse principalement `a la d´etection de contours, ces contours ´etant d´eﬁnis
comme des zones de fort contraste de signaux visuels rec¸us.
— L’aire V2 rec¸oit les informations de V1 et extrait des informations telles que la fr´equence spatiale,
l’orientation, ou encore la couleur.
— L’aire V4, qui rec¸oit des informations de V2, mais aussi de V1 directement, d´etecte des caract´eristiques
plus complexes et abstraites li´ees par exemple `a la forme.
— Le gyrus temporal inf´erieur est charg´e de la partie s´emantique (reconnaissance des objets), `a partir
des informations rec¸ues des aires pr´ec´edentes et d’une m´emoire des informations stock´ees sur des
objets.
L’architecture et le fonctionnement des r´eseaux convolutifs sont inspir´es par ces processus biologiques.
Ces r´eseaux consistent en un empilage multicouche de perceptrons, dont le but est de pr´etraiter de petites
quantit´es d’informations. Les r´eseaux convolutifs ont de larges applications dans la reconnaissance d’image
et vid´eo, les syst`emes de recommandation et le traitement du langage naturel (voir section 6- pour quelques
exemples)
Un r´eseau convolutif se compose de deux types de neurones, agenc´es en couches traitant successivement
l’information. Dans le cas du traitement de donn´ees de type images, on a ainsi :
— des neurones de traitement, qui traitent une portion limit´ee de l’image (le champ r´eceptif) au travers
d’une fonction de convolution;
— des neurones de mise en commun des sorties dits d’agr´egation totale ou partielle (pooling).
Un traitement correctif non lin´eaire est appliqu´e entre chaque couche pour am´eliorer la pertinence du
r´esultat. L’ensemble des sorties d’une couche de traitement permet de reconstituer une image interm´ediaire,
dite carte de caract´eristiques (feature map), qui sert de base `a la couche suivante. Les couches et leurs
connexions apprennent des niveaux d’abstraction croissants et extraient des caract´eristiques de plus en plus
haut niveau des donn´ees d’entr´ee.
Dans la suite, le propos sera illustr´e sur des images 2D en niveaux de gris, de taille n1 ×n2 :
I : [[1···n1]]×[[1···n2]]
→
R
(i, j)
7→
Ii,j
I sera indiff´eremment vue comme une fonction ou une matrice.
1.2-
Convolution discr`ete
Pour reproduire la notion de champ r´eceptif, et ainsi permettre aux neurones de d´etecter des caract´eristiques
de petite taille mais porteurs d’information, l’id´ee est de laisser un neurone cach´e voir et traiter seulement
une petite portion de l’image qu’il prend en entr´ee. L’outil retenu dans les r´eseaux convolutifs est la convo-
lution discr`ete.
D´eﬁnition 1-.1 (Convolution discr`ete) Soient h1,h2 ∈N,K ∈R(2h1+1)×(2h2+1). La convolution
discr`ete de I par le ﬁltre K est donn´ee par :
h2
∑
v=−h2
Ku,vIr+u,s+v
(.1)
o`u K est donn´e par :
K−h1,−h2
...
K−h1,h2
...
K0,0
...
Kh1,−h2
...
Kh1,h2


.
(.2)
La taille du ﬁltre (2h1 +1)×(2h2 +1) pr´ecise le champ visuel captur´e et trait´e par K.
Lorsque K parcourt I, le d´eplacement du ﬁltre est r´egl´e par deux param`etres de stride (horizontal et vertical).
Un stride de 1 horizontal (respectivement vertical) signiﬁe que K se d´eplace d’une position horizontale
(resp. verticale) `a chaque application de .1. Les valeurs de stride peuvent ´egalement ˆetre sup´erieures et ainsi
sous-´echantillonner I.
Le comportement du ﬁltre sur les bords de I doit ´egalement ˆetre pr´ecis´e, par l’interm´ediaire d’un param`etre
de padding. Si l’image convolu´ee (K∗I) doit poss´eder la mˆeme taille que I, alors 2h1 lignes de 0 (h1 en
haut et h1 en bas) et 2h2 colonnes de 0 (h2 `a gauche et h2 `a droite) doivent ˆetre ajout´ees. Dans le cas o`u la
convolution est r´ealis´ee sans padding, l’image convolu´ee est de taille (n1 −2h1)×(n2 −2h2).
2-
D´EFINITION DES COUCHES
Nous introduisons ici les diff´erents types de couches utilis´ees dans les r´eseaux convolutifs. L’assemblage de
ces couches permet de construire des architectures complexes pour la classiﬁcation ou la r´egression, dont
certaines seront pr´ecis´ees dans un prochain cours.
2.1-
Couche de convolution
Soit l ∈N une couche de convolution. L’entr´ee de la couche l est compos´ee de n(l−1) cartes provenant de
la couche pr´ec´edente, de taille n(l−1)
1
×n(l−1)
2
. Dans le cas de la couche d’entr´ee du r´eseau (l = 1), l’entr´ee
est l’image I. La sortie de la couche l est form´ee de n(l) cartes de taille n(l)
1 ×n(l)
2 . La ie carte de la couche
l, not´ee Y(l)
i , se calcule comme :
Y(l)
i
= B(l)
i +
n(l−1)
∑
j=1
K(l)
i,j ∗Y(l−1)
j
(.3)
o`u B(l)
i
est une matrice de biais et K(l)
i,j est le ﬁltre de taille (2h(l)
1 +1)×(2h(l)
2 +1) connectant la je carte de
la couche (l −1) `a la ie carte de la couche l (voir la ﬁgure 2-2).
n(l)
1 et n(l)
2 doivent prendre en compte les effets de bords : lors du calcul de la convolution, seuls les pixels
dont la somme est d´eﬁnie avec des indices positifs doivent ˆetre trait´es. Dans le cas o`u le padding n’est pas
utilis´e, les cartes de sortie ont donc une taille de n(l)
1 = n(l−1)
1
−2h(l)
1 et n(l)
2 = n(l−1)
2
−2h(l)
2 .
(1,8)
Y (l−1)
i
(2,8)
Y (l−1)
i
(3,8)
Y (l−1)
i
(4,8)
Y (l−1)
i
(5,8)
Y (l−1)
i
(6,8)
Y (l−1)
i
(7,8)
Y (l−1)
i
(8,8)
Y (l−1)
i
FIGURE 2-1 – Illustration des calculs effectu´es dans une op´eration de convolution discr`ete. Le pixel (2,2)
de l’image Y(l)
i
est une combinaison lin´eaire des pixels (i, j),i, j ∈[[1,3]] de Y(l−1)
i
, les coefﬁcients de la
combinaison ´etant port´es par le ﬁltre K (´equation .3).
Souvent, les ﬁltres utilis´es pour calculer Y(l)
i
sont les mˆemes, i.e. K(l)
i,j = K(l)
i,k pour j , k. De plus, la somme
dans l’´equation (.3) peut ˆetre conduite sur un sous ensemble des cartes d’entr´ee.
Il est possible de mettre en correspondance la couche de convolution et l’op´eration (.3) qu’elle effectue, avec
un perceptron multicouche. Pour cela, il sufﬁt de r´e´ecrire l’´equation (.3) : chaque carte Y(l)
i
de la couche l
est form´ee de n(l)
1 ·n(l)
2 neurones organis´es dans un tableau `a deux dimensions. Le neurone en position (r,s)
calcule :
r,s =

B(l)
i

r,s +
n(l−1)
∑
j=1

K(l)
i,j ∗Y(l−1)
j

=

B(l)
i

r,s +
n(l−1)
∑
j=1

K(l)
i,j


Y(l−1)
j

r+u,s+v
(.5)
Les param`etres du r´eseau `a entraˆıner (poids) peuvent alors ˆetre trouv´es dans les ﬁltres K(l)
i,j et les matrices
de biais B(l)
i .
Comme nous le verrons dans la section 2.4-, un sous-´echantillonnage est utilis´e pour diminuer l’inﬂuence
du bruit et des distorsions dans les images. Le sous-´echantillonnage peut ˆetre ´egalement r´ealis´e simplement
avec des param`etres de stride, en sautant un nombre ﬁxe de pixels dans les dimensions horizontale (saut
s(l)
1 ) et verticale (saut s(l)
2 ) avant d’appliquer de nouveau le ﬁltre. La taille des images de sortie est alors :
n(l)
1 = n(l−1)
1
−2h(l)
1
s(l)
1 +1
et
n(l)
2 = n(l−1)
2
−2h(l)
2
s(l)
2 +1
.
(.6)
Un point cl´e des r´eseaux convolutifs est d’exploiter la corr´elation spatiale des donn´ees. L’utilisation des
noyaux permet d’all´eger le mod`ele, plutˆot que d’utiliser des couches compl`etement connect´ees.
FIGURE 2-2 – Illustration d’une couche de
convolution l. L’image d’entr´ee (l = 1) ou
une carte de caract´eristiques de la couche
(l −1) est convolu´ee par diff´erents ﬁltres
pour donner les cartes de sortie de la couche
l.
2.2-
Couche non lin´eaire
Pour augmenter le pouvoir d’expression des r´eseaux profonds, on utilise des couches non lin´eaires. Les
entr´ees d’une couche non lin´eaire sont n(l−1) cartes et ses sorties n(l) = n(l−1) cartes Y(l)
i , de taille n(l−1)
1
×
n(l−1)
2
telles que n(l)
1 = n(l−1)
1
et n(l)
2 = n(l−1)
2
, donn´ees par Y(l)
i
= f

Y(l−1)
i

, o`u f est la fonction d’activation
utilis´ee dans la couche l. Le tableau .1 propose quelques fonctions d’activation usuelles.
En apprentissage profond, il a ´et´e report´e que la sigmo¨ıde et la tangente hyperbolique avaient des perfor-
mances moindres que la fonction d’activation softsign :
1+
Y(l−1)
i
.
(.7)
En effet, les valeurs des pixels des cartes Y(l−1)
i
arrivant pr`es des paliers de saturation de ces fonctions
donnent des gradients faibles, qui ont tendance `a s’annuler (probl`eme du gradient ´evanescent ou vanishing
gradient) lors de la phase d’apprentissage par r´etropropagation du gradient. Une autre fonction, non satu-
rante elle, est tr`es largement utilis´ee. Il s’agit de la fonction ReLU (Rectiﬁed Linear Unit) [9] :
Y(l)
i
= max

0,Y(l−1)
i

(.8)
Les neurones utilisant la fonction d´ecrite dans l’´equation (.8) sont appel´es neurones lin´eaires rectiﬁ´es. Glo-
rot et Bengio [5] ont montr´e que l’utilisation d’une couche ReLU en tant que couche non lin´eaire permettait
un entraˆınement efﬁcace de r´eseaux profonds sans pr´e-entraˆınement non supervis´e. Plusieurs variantes de
cette fonction existent, par exemple pour assurer une diff´erentiabilit´e en 0 ou pour proposer des valeurs
non nulles pour des valeurs n´egatives de l’argument. La ﬁgure 2-3. illustre quelques unes de ces fonctions
d’activation.
2.3-
Couches de normalisation
La normalisation prend aujourd’hui une place de plus en plus importante, notamment depuis les travaux
de Ioffe et Szegedy [8]. Les auteurs sugg`erent qu’un changement dans la distribution des activations d’un
r´eseau profond, r´esultant de la pr´esentation d’un nouveau mini batch d’exemples, ralentit le processus d’ap-
prentissage. Pour pallier ce probl`eme, chaque activation du mini batch est centr´ee et norm´ee (variance
unit´e), la moyenne et la variance ´etant calcul´ees sur le mini batch entier, ind´ependamment pour chaque
activation. Des param`etres d’offset β et multiplicatif γ sont alors appliqu´es pour normaliser les donn´ees
d’entr´ee (algorithme 1).
Lorsque la descente de gradient est achev´ee, un post apprentissage est appliqu´e dans lequel la moyenne et
la variance sont calcul´ees sur l’ensemble d’entraˆınement et remplacent µB et σ2
B (algorithme 2).
Nom
Graphe
f
f ′
Rampe
f(x) = x
f ′(x) = 1
Heaviside
f(x) =
(
0
si x < 0
1
si x ≥0
f ′(x) =
(
0
si x , 0
?
si x = 0
Logistique ou
sigmo¨ıde
f(x) =
1
1+e−x
f ′(x) = f(x)

1 −f(x)

Tangente
hyperbolique
f(x) = tanh(x)
=
2
1+e−2x −1
f ′(x) = 1 −f 2(x)
Arc Tangente
f(x) = tan−1(x)
f ′(x) =
1
x2 +1
ReLU
f(x) =
(
0
si x < 0
x
si x ≥0
f ′(x) =
(
0
si x , 0
1
si x = 0
Exponentielle
Lin´eaire
f(x) =
(
α(ex −1)
si x < 0
x
si x ≥0
f ′(x) =
(
f(x) + α
si x < 0
1
si x ≥0
TABLE .1 – Quelques fonctions d’activation
2.4-
Couche d’agr´egation et de sous-´echantillonnage
Le sous-´echantillonnage (pooling) des cartes obtenues par les couches pr´ec´edentes a pour objectif d’assurer
une robustesse au bruit et aux distorsions.
La sortie d’une couche d’agr´egation l (ﬁgure 2-4) est compos´ee de n(l) = n(l−1) cartes de taille r´eduite.
En g´en´eral, l’agr´egation est effectu´ee en d´eplac¸ant dans les cartes d’entr´ee une fenˆetre de taille 2p × 2p
toutes les q positions (il y a recouvrement si q < p et non recouvrement sinon), et en calculant, pour chaque
position de la fenˆetre, une seule valeur, affect´ee `a la position centrale dans la carte de sortie. On distingue
g´en´eralement deux types d’agr´egation :
−1
1
2
3
4
5
ReLU(x) = max(0,x)
LeakyReLU(x,α) =
(
x
si x > 0
αx
sinon
(
x
si x > 0
α(ex −1)
sinon
SeLU(x,α,λ) =
(
λx
si x > 0
λα(ex −1)
sinon
FIGURE 2-3 – Quelques fonctions d’activation
Algorithme 1 : Normalisation par batch sur la pr´esentation d’un mini batch B
Donn´ees : valeurs de l’activation x sur un mini batch B = {x1 ···xm}
Param`etres β,γ `a apprendre
R´esultat : Donn´ees normalis´ees {y1 ···ym} = BNγ,β(x1 ···xm)
d´ebut
m
∑
i=1
(xi −µB)2
pour i=1 `a m faire
yi = γ xi−µB
q
La moyenne : on utilise un ﬁltre KB de taille (2h1 +1)×(2h2 +1) d´eﬁni par :
(KB)r,s =
1
(2h1 +1)(2h2 +1)
Algorithme 2 : Normalisation par batch d’un r´eseau
Donn´ees : un r´eseau N
un ensemble d’activations {x1 ···xK}
d´ebut
Nn = N
pour i=1 `a K faire
Calculer yi = BNγ,β(xi) `a l’aide de l’algorithme 1
Modiﬁer chaque couche de Nn : l’entr´ee yi remplace l’entr´ee xi
Entraˆıner Nn pour optimiser les param`etres de N et (γi,βi)1≤i≤K
N f = Nn
pour i=1 `a K faire
Utiliser N f sur des batchs B de taille m
Calculer la moyenne des moyennes ¯xi et des variances Var(xi)
Remplacer dans N f la transformation yi = BNγ,β(xi) par
Var(xi)+ε
xi +
βi −
γi ¯xi
p
FIGURE 2-4
–
Couche
d’agr´egation
et
de
sous-
´echantillonnage l. Chacune des n(l−1) cartes de la
couche l −1 est trait´ee individuellement. Chaque neurone
des n(l) = n(l−1) cartes de sortie est la moyenne (ou
le maximum) des valeurs contenues dans une fenˆetre
de taille donn´ee dans la carte correspondante de la
couche (l −1).
Le maximum : la valeur maximum dans la fenˆetre est retenue.
Le maximum est souvent utilis´e pour assurer une convergence rapide durant la phase d’entraˆınement.
L’agr´egation avec recouvrement, elle, semble assurer une r´eduction du ph´enom`ene de surapprentissage
2.5-
Couche compl`etement connect´ee
Si l et (l −1) sont des couches compl`etement connect´ees, l’´equation :
z(l)
i
=
m(l−1)
∑
k=0
w(l)
i,ky(l−1)
k
ou
Z(l) = W(l)Y(l−1)
(.9)
avec Z(l), W(l) et Y(l−1) les repr´esentations vectorielle et matricielle des entr´ees z(l)
i , des poids w(l)
i,k et des
sorties y(l−1)
k
, permet de relier ces deux couches.
Dans le cas contraire, la couche l attend n(l−1) entr´ees de taille n(l−1)
1
×n(l−1)
2
et le ie neurone de la couche
l calcule :
y(l)
i
= f

z(l)
i

avec
z(l)
i
=
n(l−1)
∑
j=1
n(l−1)
1
∑
r=1
n(l−1)
2
∑
s=1
w(l)
i,j,r,s

Y(l−1)
j

o`u w(l)
i,j,r,s est le poids connectant le neurone en position (r,s) de la je carte de la couche (l −1) au ie neurone
de la couche l.
En pratique, les r´eseaux convolutifs sont utilis´es pour apprendre une hi´erarchie dans les donn´ees et la
(ou les) couche(s) compl`etement connect´ee(s) est(sont) utilis´ee(s) en bout de r´eseau pour des tˆaches de
classiﬁcation ou de r´egression.
Une couche de classiﬁcation classiquement mise en œuvre utilise le classiﬁeur softmax, qui g´en´eralise la
r´egression logistique au cas multiclasse (k classes). L’ensemble d’apprentissage Ea =
n
(x(i),y(i)),i ∈[[1···m]]
o
est donc tel que y(i) ∈[[1···k]] et le classiﬁeur estime la probabilit´e P(y(i) = j|x(i)) pour chaque classe
1 ≤j ≤k. Le classiﬁeur softmax calcule cette probabilit´e selon :
∀j ∈[[1···k]]
P(y(i) = j|x(i),W) =
eW⊤
j x(i)
k
∑
l=1
eW⊤
l x(i)
(.11)
o`u W est la matrice des param`etres du mod`ele (les poids). Ces param`etres sont obtenus en minimisant une
fonction de coˆut, qui peut par exemple s’´ecrire :
k
∑
j=1
Iy(i)=jlog
k
∑
l=1
eW⊤
l x(i)





+ λ
k
∑
j=1
W 2
ji
(.12)
o`u λ est un param`etre de r´egularisation contrˆolant le second terme du coˆut qui p´enalise les grandes valeurs
des poids (r´egularisation ℓ2).
3-
R´EGULARISATION
Un des enjeux principaux en apprentissage automatique est de construire des algorithmes ayant une bonne
capacit´e de g´en´eralisation. Les strat´egies mises en œuvre pour arriver `a cette ﬁn rentrent dans la cat´egorie
g´en´erale de la r´egularisation et de nombreuses m´ethodes sont aujourd’hui propos´ees en ce sens. Nous
faisons ici un focus sur trois strat´egies largement utilis´ees en apprentissage profond.
3.1-
R´egularisation de la fonction de coˆut
L’´equation .12 est un exemple de r´egularisation de la fonction de coˆut, utilis´ee lors de la phase d’en-
traˆınement. `A la fonction d’erreur est ajout´ee une fonction des poids du r´eseau, qui peut prendre de multiples
formes. Les deux principales strat´egies sont :
— La r´egularisation ℓ2 (ou ridge regression), qui force les poids `a avoir une faible valeur absolue : un
terme de r´egularisation fonction de la norme ℓ2 de la matrice des poids est ajout´e (`a la mani`ere de
l’´equation .12). On parle souvent de weight decay.
— La r´egularisation ℓ1, qui tend `a rendre ´epars le r´eseau profond, i.e. `a imposer `a un maximum de
poids de s’annuler. Un terme de r´egularisation, somme pond´er´ee des valeurs absolues des poids, est
ajout´e `a la fonction objectif.
3.2-
Dropout
Les techniques de dropout se rapprochent des strat´egies classiques de bagging en apprentissage automa-
tique. L’objectif est d’entraˆıner un ensemble constitu´e de tous les sous-r´eseaux qui peuvent ˆetre construits
en supprimant des neurones (hors neurones d’entr´ee et de sortie) du r´eseau initial. Si le r´eseau comporte |W|
neurones cach´es, il existe ainsi 2|W| mod`eles possibles. En pratique, les neurones cach´es se voient perturb´es
par un bruit binomial, qui a pour effet de les empˆecher de fonctionner en groupe et de les rendre, au contraire,
plus ind´ependants. Le ph´enom`ene de surapprentissage est ainsi fortement r´eduit sur le r´eseau, qui doit
d´ecomposer les entr´ees en caract´eristiques pertinentes, ind´ependamment les unes des autres. Les r´eseaux
construits par dropout partagent partiellement leurs param`etres, ce qui diminue l’empreinte m´emoire de la
m´ethode.
Lors de la phase de pr´ediction, le r´eseau complet est utilis´e, mais les neurones cach´es sont pond´er´es par
la fraction de bruit utilis´e pendant l’apprentissage (i.e. pour chaque neurone le nombre de fois o`u il a ´et´e
supprim´e d’un sous-r´eseau, rapport´e au nombre total de r´eseaux), aﬁn de conserver la valeur moyenne des
activations des neurones identiques `a celles durant l’apprentissage.
Notons qu’il est ´egalement possible d’´eteindre non pas un neurone, mais un poids. La strat´egie correspon-
dante est appel´ee DropConnect.
3.3-
Partage de param`etres
La r´egularisation de la fonction de coˆut permet d’imposer aux poids certaines contraintes (par exemple
de rester faibles en amplitude pour la r´egularisation ℓ2, ou de s’annuler pour la r´egularisation ℓ1). Il peut
´egalement ˆetre int´eressant d’imposer certains a priori sur les poids, par exemple une d´ependance entre les
valeurs des param`etres.
Une d´ependance classique consiste `a imposer que les valeurs de certains poids soient proches les unes des
autres (dans le cas par exemple o`u deux mod`eles de classiﬁcation M1 et M2, de param`etres W1 et W2,
op`erent sur des donn´ees similaires et sur des classes identiques) et, l`a encore, une strat´egie de p´enalisation
de la fonction objectif peut ˆetre utilis´ee. Cependant, il est plus courant dans ce cas d’imposer que les
param`etres soient ´egaux (dans l’exemple pr´ec´edent imposer W1 = W2) et d’arriver `a une strat´egie dite de
partage des param`etres. Dans le cas des r´eseaux convolutifs utilis´es en vision, cette r´egularisation est assez
intuitive puisque les entr´ees (images) poss`edent de nombreuses propri´et´es invariantes par transformations
afﬁnes (une image de voiture reste une image de voiture, mˆeme si l’image est translat´ee ou mise `a l’´echelle,
cf. ﬁgure 3-5). Le r´eseau exploite alors ce partage de param`etres, en calculant une mˆeme caract´eristique
(un neurone et son poids) `a diff´erentes positions dans l’image. De ce fait, le nombre de param`etres est
drastiquement r´eduit, ainsi que l’empreinte m´emoire du r´eseau appris.
image
neurones
FIGURE 3-5 – Partage de param`etres : les neurones voient
des champs r´eceptifs distincts, mais partagent les mˆemes pa-
ram`etres (poids). Leur capacit´e de d´etection d’un triangle
restera la mˆeme, quelle que soit la position de l’objet dans
l’image.
4-
INITIALISATION
Une initialisation convenable des poids est essentielle pour assurer une convergence de la phase d’en-
traˆınement. Un choix arbitraire des poids (`a z´ero, `a de petites ou grandes valeurs al´eatoires) peut ralentir,
voire causer de la redondance dans le r´eseau (probl`eme de la sym´etrie).
Plusieurs sch´emas d’initialisation ont ´et´e propos´es et nous donnons dans les deux paragraphes qui suivent
d’eux d’entre eux.
4.1-
Initialisation prenant en compte les variations des neurones
Pour illustrer le propos, on suppose que l’entr´ee du r´eseau de neurones est compos´ee de n(1) entr´ees x =
(x1 ...xn(1))⊤, les xi ´etant i.i.d, normalis´es selon une loi N (0,σx). Pour simpliﬁer la d´emonstration, on
suppose que la couche suivante calcule un simple potentiel post synaptique : pour un neurone de cette
couche, ce potentiel est w⊤x, avec w ∈Rn(1) i.i.d N (0,σw). La variance de ce potentiel est alors
Var(w⊤x) =
n(1)
∑
i=1
Var(wixi) =
n(1)
∑
i=1

E(wi)2Var(xi)+E(xi)2Var(wi)+Var(wi)Var(xi)

Les entr´ees et les poids sont de moyenne nulle, donc
Var(w⊤x) =
n(1)
∑
i=1
σxσw
et puisque les wixi sont i.i.d.
Var(w⊤x) = n(1)σxσw
On montre plus g´en´eralement que sur la le couche cach´ee, la variance de Y(l) est
Var(Y(l)) =

n(l)Var(wi)
l
Var(xi)
Chaque neurone peut donc varier dans un rapport de n(l) fois la variation de son entr´ee (qui est elle mˆeme
n(l−1) fois la variance de son entr´ee...)
On a alors les cas de ﬁgure suivants :
— si n(l)Var(wi) > 1 le gradient va tendre vers de grandes valeurs `a mesure que l’on s’enfonce dans le
r´eseau (que l croˆıt)
— si n(l)Var(wi) < 1 le gradient disparaˆıt `a mesure que l’on s’enfonce dans le r´eseau
Pour ´eviter ces deux probl`emes, la solution est de forcer n(l)Var(wi) = 1, soit Var(wi) = 1/n(l). On initialise
donc les poids se la couche l selon une loi
n(l−1) N (0,1)
Cette proc´edure est la m´ethode d’initialisation de Xavier, ou de Glorot [4].
4.2-
Initialisation de He
He a montr´e dans [7] que la m´ethode de Xavier pouvait ne pas fonctionner correctement lorsque les traite-
ments non lin´eaires ´etaient effectu´es par la fonction ReLU. Les auteurs proposent alors de plutˆot multiplier
par
√
n(l−1) pour prendre en compte la partie n´egative qui ne participe pas au calcul de la variance.
5-
APPRENTISSAGE
La pr´esence de nombreuses couches cach´ees va permettre de calculer des caract´eristiques beaucoup plus
complexes et informatives des entr´ees. Chaque couche calculant une transformation non lin´eaire de la
couche pr´ec´edente, le pouvoir de repr´esentation de ces r´eseaux s’en trouve am´elior´e. On peut par exemple
montrer qu’il existe des fonctions qu’un r´eseau `a k couches peut repr´esenter de mani`ere compacte (avec un
nombre de neurones cach´es qui est polynomial en le nombre des entr´ees), alors qu’un r´eseau `a k−1 couches
ne peut pas le faire, `a moins d’avoir une combinatoire exponentielle sur le nombre de neurones cach´es.
5.1-
Le probl`eme de l’entraˆınement
Si l’int´erˆet de ces r´eseaux est manifeste, la complexit´e de leur utilisation vient de l’´etape d’apprentissage.
Jusqu’`a r´ecemment, l’algorithme utilis´e ´etait classique et consistait en une initialisation al´eatoire des poids
du r´eseau, suivie d’un entraˆınement sur un ensemble d’apprentissage, en minimisant une fonction objectif.
Cependant, dans le cas des r´eseaux profonds, cette approche peut ne pas ˆetre adapt´ee :
— Les donn´ees ´etiquet´ees doivent ˆetre en nombre sufﬁsant pour permettre un entraˆınement efﬁcace,
d’autant plus que le r´eseau est complexe. Dans le cas contraire, un surapprentissage peut notamment
ˆetre induit.
— Sur un tel r´eseau, l’apprentissage se r´esume `a l’optimisation d’une fonction fortement non convexe,
qui am`ene presque sˆurement `a des minima locaux lorsque des algorithmes classiques sont utilis´es.
— Dans l’´etape de r´etropropagation, les gradients diminuent rapidement `a mesure que le nombre de
couches cach´ees augmente. La d´eriv´ee de la fonction objectif par rapport `a W devient alors tr`es
faible `a mesure que le calcul se r´etropropage vers la couche d’entr´ee. Les poids des premi`eres
couches changent donc tr`es lentement et le r´eseau n’est plus en capacit´e d’apprendre. Ce probl`eme
est connu sous le nom de probl`eme de gradient ´evanescent (vanishing gradient).
L’algorithme principalement utilis´e pour l’apprentissage des r´eseaux convolutifs reste la r´etropropagation
du gradient. Le choix de la fonction objectif, de sa r´egularisation, de la m´ethode d’optimisation (descente
de gradient, m´ethodes `a taux d’apprentissage adaptatifs telles qu’AdaGrad, RMSProp ou Adam) et des
param`etres associ´es, ou des techniques de pr´esentation des exemples (batchs, minibatchs) sont autant de
facteurs importants permettant aux mod`eles non seulement de converger vers un optimum local satisfaisant,
mais ´egalement de proposer un mod`ele ﬁnal ayant une bonne capacit´e de g´en´eralisation.
Aujourd’hui, de nombreux r´eseaux, d´ej`a entraˆın´es, sont mis `a disposition. En effet, ces entraˆınements
n´ecessitent de grandes bases d’apprentissage (type ImageNet) et une puissance de calcul assez ´elev´ee (GPU
obligatoire(s)). Pour le traitement de probl`emes pr´ecis, des m´ethodes existent, qui partent de ces r´eseaux
pr´eentraˆın´es et les modiﬁent localement pour, par exemple, apprendre de nouvelles classes d’images non en-
core vues par le r´eseau. L’id´ee sous-jacente est que les premi`eres couches capturent des caract´eristiques bas
niveau et que la s´emantique vient avec les couches profondes. Ainsi, dans un probl`eme de classiﬁcation, o`u
les classes n’ont pas ´et´e apprises, on peut supposer qu’en conservant les premi`eres couches on extraira des
caract´eristiques communes des images (bords, colorim´etrie,...) et qu’en changeant les derni`eres couches (in-
formation s´emantique et haut niveau et ´etage de classiﬁcation), c’est-`a-dire en r´eapprenant les connexions,
on sp´eciﬁera le nouveau r´eseau pour la nouvelle tˆache de classiﬁcation. Cette approche rentre dans le cadre
des m´ethodes de transfer learning [10] et de ﬁne tuning, cas particulier d’adaptation de domaine :
— Les m´ethodes de transfert prennent un r´eseau d´ej`a entraˆın´e, enl`event la derni`ere couche compl`etement
connect´ee et traitent le r´eseau restant comme un extracteur de caract´eristiques. Un nouveau classi-
ﬁeur, la derni`ere couche, est alors entraˆın´e sur le nouveau probl`eme.
— Les m´ethodes de ﬁne tuning r´e-entraˆınent le classiﬁeur du r´eseau et remettent `a jour les poids du
r´eseau pr´e-entraˆın´e par r´etropropagation.
Plusieurs facteurs inﬂuent sur le choix de la m´ethode `a utiliser : la taille des donn´ees d’apprentissage du
nouveau probl`eme et la ressemblance du nouveau jeu de donn´ees avec celui qui a servi `a entraˆıner le r´eseau
initial :
— Pour un jeu de donn´ees similaire de petite taille, on utilise du transfer learning, avec un classiﬁeur
utilis´e sur les caract´eristiques calcul´ees sur les derni`eres couches du r´eseau initial.
— Pour un jeu de donn´ees de petite taille et un probl`eme diff´erent, on utilise du transfer learning, avec
un classiﬁeur utilis´e sur les caract´eristiques calcul´ees sur les premi`eres couches du r´eseau initial.
— Pour un jeu de donn´ees, similaire ou non, de grande taille, on utilise le ﬁne tuning.
Notons qu’il est toujours possible d’augmenter la taille du jeu de donn´ees par des technique d’augmentation
de donn´ees.
Dans le cas o`u un r´eseau ad hoc doit ˆetre construit et o`u une base d’apprentissage sufﬁsante est disponible,
l’entraˆınement par optimisation reste possible. Il existe en particulier des techniques d’apprentissage couche
`a couche, lorsque les couches successives calculent des fonctions d’activation des couches pr´ec´edentes
(empilement d’autoencodeurs par exemple).
5.2-
Apprentissage glouton par couche
L’id´ee est d’entraˆıner les couches une `a une, d’abord dans un r´eseau `a une couche cach´ee, puis `a deux
couches cach´ees... `A chaque ´etape k, la couche k est ajout´ee et a pour entr´ee la couche k−1 pr´ec´edemment
entraˆın´ee. L’entraˆınement peut ˆetre supervis´e, mais le plus souvent il est non supervis´e. Les poids issus de
cet entraˆınement servent d’initialisation pour le r´eseau ﬁnal.
En comparaison des points pr´ec´edents, cette approche est bien plus pertinente :
— Les donn´ees non ´etiquet´ees sont tr`es faciles `a obtenir.
— L’initialisation des poids sur des donn´ees non ´etiquet´ees est plus performante qu’une initialisation
al´eatoire. Empiriquement, une m´ethode type descente de gradient permet d’aboutir `a un meilleur
minimum local (les donn´ees non ´etiquet´ees fournissent en effet des informations a priori d´ej`a im-
portantes sur les donn´ees).
5.3-
Visualisation du m´ecanisme des r´eseaux convolutifs
Le m´ecanisme interne des r´eseaux convolutifs est mal compris et l’analyse des raisons qui font que leur
puissance de pr´ediction est importante n’est pas ais´ee. S’il est toujours possible de r´etroprojeter les acti-
vations depuis la premi`ere couche de convolution, les couches d’agr´egation et de rectiﬁcation empˆechent
de comprendre le fonctionnement des couches suivantes, ce qui peut ˆetre gˆenant dans la construction et
l’am´elioration de ces r´eseaux.
Les m´ethodes de visualisation du fonctionnement des r´eseaux convolutifs peuvent ˆetre rang´ees en trois
cat´egories, d´ecrites dans les paragraphes suivants.
5.4-
M´ethodes de visualisation de base
Les m´ethodes les plus simples consistent `a visualiser les activations lors du passage d’une image dans le
r´eseau. Pour des activations type ReLU, ces activations sont ininterpr´etables au d´ebut de l’entraˆınement,
mais `a mesure que ce dernier progresse, les cartes d’activation Y(l)
i
deviennent localis´ees et ´eparses.
Il est ´egalement possible de visualiser les ﬁltres des diff´erentes couches de convolution (ﬁgure 5-6). Les
ﬁltres des premi`eres couches agissent comme des d´etecteurs de bords et coins et, `a mesure que l’on s’en-
fonce dans le r´eseau, les ﬁltres capturent des concepts haut niveau comme des objets ou encore des visages.
Citons encore d’autres m´ethodes qui proposent de visualiser les derni`eres couches (les couches compl`etement
connect´ees) de grande dimension (par exemple 4096 pour AlexNet) via une m´ethode de r´eduction de di-
mension.
FIGURE 5-6 – Visualisation des ﬁltres de la premi`ere couche d’AlexNet (`a gauche, 64 ﬁltres 11×11) et de
ResNet-18 (`a droite, 64 ﬁltres 7×7).
5.5-
M´ethodes fond´ees sur les activations
Plusieurs strat´egies peuvent ˆetre adopt´ees pour sonder le fonctionnement d’un r´eseau convolutif, en utilisant
les informations port´ees par les cartes Y(l)
i , parmi lesquelles :
— Utiliser des couches de convolution transpos´ee (improprement appel´ees parfois couches de d´econvolution),
ajout´ees `a chaque couche de convolution du r´eseau. ´Etant donn´ees les cartes d’entr´ee de la couche l,
les cartes de sortie Y(l)
i
sont envoy´ees dans la couche de convolution transpos´ee correspondante au
niveau l. Cette derni`ere reconstruit les Y(l−1)
i
qui ont permis le calcul des activations de la couche l.
Le processus est alors it´er´e jusqu’`a atteindre la couche d’entr´ee l = 1, les activations de la couche l
´etant alors r´etroprojet´ees dans le plan image [13]. La pr´esence de couches d’agr´egation et de rectiﬁ-
cation rend ce processus non inversible (par exemple, une couche d’agr´egation maximum n´ecessite
de connaˆıtre `a quelles positions de l’image Y(l)
i
sont situ´es les maxima retenus).
— Faire passer un grand nombre d’images dans le r´eseau et, pour un neurone particulier, conserver
celle qui a le plus activ´e ce neurone. Il est alors possible de visualiser les images pour comprendre
ce `a quoi le neurone s’int´eresse dans son champ r´eceptif (ﬁgure 5-7).
FIGURE 5-7 – Champ r´ec´eptif de quelques neurones de la derni`ere couche d’agr´egation du r´eseau AlexNet,
superpos´ees aux images ayant le plus fortement activ´e ces neurones. Le champ est encadr´e en blanc, et la
valeur d’activation correspondante est report´ee en haut. On voit par exemple que certains neurones sont tr`es
sensibles aux textes, d’autres aux r´eﬂexions sp´eculaires, ou encore aux hauts du corps (source : [3]).
— Cacher (par un rectangle noir par exemple) diff´erentes parties de l’image d’entr´ee qui est d’une
certaine classe (disons un chien) et observer la sortie du r´eseau (la probabilit´e de la classe de l’image
d’entr´ee). En repr´esentant les valeurs de probabilit´e de la classe d’int´erˆet comme une fonction de
la position du rectangle occultant, il est possible de voir si le r´eseau s’int´eresse effectivement aux
parties de l’image sp´eciﬁques de la classe, ou `a des autres zones (le fond par exemple) (ﬁgure 5-8).
5.6-
M´ethodes fond´ees sur le gradient
Pour comprendre quelle(s) partie(s) de l’image est (sont) utilis´ee(s) par le r´eseau pour effectuer une pr´ediction,
il est possible de calculer des cartes de saillance (saliency maps). L’id´ee est relativement simple : calculer le
gradient de la classe de sortie par rapport `a l’image d’entr´ee. Cela indique `a quel point une petite variation
dans l’image induit un changement de pr´ediction. En visualisant les gradients, on observe alors par exemple
leurs fortes valeurs, indiquant qu’une petite variation du pixel correspondant augmente la valeur de sortie.
Il est ´egalement possible d’utiliser le gradient par rapport `a la derni`ere couche de convolution (approche
Grad-CAM), ce qui permet de r´ecup´erer des informations de localisation spatiale des r´egions importantes
pour la pr´ediction (ﬁgure 5-9, droite).
Plus g´en´eralement, en choisissant un neurone interm´ediaire du r´eseau (d’une couche de convolution), la
m´ethode de r´etropropagation guid´ee calcule le gradient de sa valeur par rapport aux pixels de l’image
FIGURE 5-8 – Occlusion d’une image (`a gauche). Le rectangle noir est d´eplac´e dans l’image et pour chaque
position la probabilit´e de la classe de l’image (ici un loulou de Pom´eranie) est enregistr´ee. Ces probabilit´es
sont ensuite repr´esent´ees sous forme d’une carte 2D (`a droite). La probabilit´e de la classe s’effondre lorsque
le rectangle couvre une partie de la face du chien. Cela sugg`ere que cette face est grandement responsable
de la forte probabilit´e de classement de l’image comme un loulou. A l’inverse, l’occlusion du fond n’alt`ere
pas la forte valeur de probabilit´e de la classe (source : [13]).
d’entr´ee, ce qui permet de souligner les parties de l’image auxquelles ce neurone r´epond (ﬁgure 5-9, mi-
lieu).
FIGURE 5-9 – Approches par gradient de visualisation du fonctionnement d’un r´eseau convolutif. Compa-
raison de la m´ethode de r´etropropagation guid´ee et de Grad-CAM.(source : [11]).
Ces gradients peuvent ´egalement ˆetre utilis´es dans la m´ethode de mont´ee de gradient (gradient Ascent),
dont l’objectif est de g´en´erer une image qui active de mani`ere maximale un neurone donn´e du r´eseau. Le
principe est d’it´erativement passer l’image d’entr´ee I dans le r´eseau pour obtenir les valeurs des cartes Y(l)
i ,
de r´etropropager pour obtenir le gradient d’un neurone par rapport aux pixels de I et d’op´erer une petite
modiﬁcation de ces pixels. Outre son aspect informatif sur la structure interne du r´eseau ´etudi´e (visualisation
des cartes Y(l)
i
interm´ediaires), cette m´ethode produit des images parfois tr`es artistiques (ﬁgure 5-10).
FIGURE 5-10 – Visualisation des 4 premi`eres couches de convolution d’un r´eseau convolutif par mont´ee de
gradient (source : [12]).
6-
QUELQUES APPLICATIONS
Depuis leur introduction en reconnaissance de caract`eres manuscrits [1], les r´eseaux convolutifs n’ont cess´e
de trouver des champs applicatifs, commerciaux et parfois ludiques. Parmi ces applications, nous en citons
ici quelques-unes.
6.0.0-.1
La classiﬁcation d’images
Premier domaine d’application des r´eseaux convolutifs, la classiﬁcation d’images consiste `a affecter une
image `a une classe, apprise par le r´eseau sur un grand nombre d’exemples. Depuis l’av`enement d’ImageNet,
et la mise en place de la comp´etition ILSVRC, les r´esultats obtenus ne cessent de s’am´eliorer et sont depuis
quelques ann´ees la r´ef´erence dans ce domaine (d´epassant mˆeme les performances humaines en 2015).
6.0.0-.2
L’annotation de sc`enes
Des r´eseaux convolutifs ont ´et´e utilis´es pour annoter des sc`enes 2D ou 2D+t, i.e. assigner `a chaque pixel un
label identiﬁant l’objet auquel il appartient. De nombreux r´eseaux ont ´et´e d´evelopp´es `a cet effet (R-CNN,
Fast R-CNN, Mast R-CNN par exemple) (ﬁgure 6-11(a)).
6.0.0-.3
La reconnaissance d’actions
Le d´eveloppement de champs r´eceptifs 3D dans les r´eseaux convolutifs a permis d’extraire dans ces r´eseaux
des caract´eristiques invariantes `a la translation. L’int´egration de techniques de r´egularisation adapt´ee (type
partage de param`etres) a rendu possible l’optimisation de ces r´eseaux pour la reconnaissance d’actions dans
des sc`enes dynamiques.
6.0.0-.4
L’analyse de documents
L’analyse de documents `a des ﬁns de reconnaissance de caract`eres, de classiﬁcation de documents ou encore
d’annotation s´emantique a largement b´en´eﬁci´e de l’apport des r´eseaux convolutifs.
6.0.0-.5
L’augmentation de donn´ees
De nombreux r´eseaux ont ´et´e propos´es pour ajouter `a des donn´ees nD des informations manquantes (colo-
risation d’images, ﬁgure 6-11(b), inpainting, restauration d’images, superr´esolution), ou pour proposer des
(a) Annotation s´emantique (source [6]).
(b) Colorisation d’images (source [14])
(c) transfert de style (source [2])
FIGURE 6-11 – Quelques applications des r´eseaux convolutifs.
versions diff´erentes des donn´ees initiales en fonction d’une contrainte de style ext´erieure (transfert de style,
ﬁgure 6-11(c)).
Les r´eseaux convolutifs sont des r´eseaux sp´ecialis´es pour traiter des donn´ees dont la topologie se conforme
`a une structure de grille n-dimensionnelle. Dans le cas de donn´ees 1D s´equentielles, d’autres r´eseaux per-
formants ont ´et´e d´evelopp´es : les r´eseaux r´ecurrents.
7-
PARTIE PRATIQUE
A l’aide du notebook fourni, r´ealisez le r´eseau convolutif de la ﬁgure 7-12.
La fonction de perte utilis´ee est l’entropie crois´ee et l’algorithme d’optimisation ADAM.
FIGURE 7-12 – Architecture du CNN `a construire
BIBLIOGRAPHIE
[1] Y. Le Cun, B. Boser, J. S. Denker, R. E. Howard, W. Habbard, L. D. Jackel, and D. Henderson.
Advances in neural information processing systems 2. chapter Handwritten Digit Recognition with a
Back-propagation Network, pages 396–404. Morgan Kaufmann Publishers Inc., San Francisco, CA,
USA, 1990.
[2] L.A. Gatys, A.S. Ecker, and M. Bethge. A neural algorithm of artistic style. CoRR, abs/1508.06576,
2015.
[3] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection
and semantic segmentation. In 2014 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), volume 00, pages 580–587, June 2014.
[4] Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neural
networks. In In Proceedings of the International Conference on Artiﬁcial Intelligence and Statistics
(AISTATS’10). Society for Artiﬁcial Intelligence and Statistics, 2010.
[5] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectiﬁer neural networks. In Geof-
frey J. Gordon, David B. Dunson, and Miroslav Dud ˜Ak, editors, AISTATS, volume 15 of JMLR
Proceedings, pages 315–323. JMLR.org, 2011.
[6] K. He, G. Gkioxari, P. Doll´ar, and R. B. Girshick. Mask R-CNN. CoRR, abs/1703.06870, 2017.
[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. CoRR, abs/1512.03385, 2015.
[8] Sergey Ioffe and Christian Szegedy. Batch normalization : Accelerating deep network training by
reducing internal covariate shift. In Francis R. Bach and David M. Blei, editors, ICML, volume 37 of
JMLR Workshop and Conference Proceedings, pages 448–456. JMLR.org, 2015.
[9] Vinod Nair and Geoffrey E. Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In
Johannes F ˜A¼rnkranz and Thorsten Joachims, editors, ICML, pages 807–814. Omnipress, 2010.
[10] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Trans. on Knowl. and Data
Eng., 22(10) :1345–1359, October 2010.
[11] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,
and Dhruv Batra. Grad-cam : Visual explanations from deep networks via gradient-based localization.
In ICCV, pages 618–626. IEEE Computer Society, 2017.
[12] Jason Yosinski, Jeff Clune, Anh Mai Nguyen, Thomas J. Fuchs, and Hod Lipson. Understanding
neural networks through deep visualization. CoRR, abs/1506.06579, 2015.
[13] Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. CoRR,
abs/1311.2901, 2013.
[14] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaolei Huang, Xiaogang Wang, and Dimi-
tris N. Metaxas. Stackgan : Text to photo-realistic image synthesis with stacked generative adversarial
networks. CoRR, abs/1612.03242, 2016.