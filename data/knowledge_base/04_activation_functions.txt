ACTIVATION FUNCTIONS
=====================

Q1: Why use activation functions in CNNs?
A: Activation functions introduce non-linearity, allowing the network to learn complex relationships. Without them, the network would be equivalent to linear regression.

Q2: What are the main activation functions?
A:
import torch.nn as nn
import torch.nn.functional as F

# ReLU (most used)
relu = nn.ReLU()
# f(x) = max(0, x)

# Leaky ReLU (avoids dead neurons)
leaky_relu = nn.LeakyReLU(negative_slope=0.01)

# GELU (used in Transformers)
gelu = nn.GELU()

# Swish/SiLU (self-gated)
silu = nn.SiLU()
# f(x) = x * sigmoid(x)

# Sigmoid (output between 0-1)
sigmoid = nn.Sigmoid()

# Tanh (output between -1 and 1)
tanh = nn.Tanh()

# Softmax (for multi-class classification)
softmax = nn.Softmax(dim=1)

Q3: Which activation to choose and when?
A:
- ReLU: Default for hidden layers (fast, efficient)
- Leaky ReLU / PReLU: If dead neurons problem with ReLU
- GELU / Swish: For modern architectures (better performance)
- Sigmoid: Last layer for binary classification
- Softmax: Last layer for multi-class classification

Q4: What is the dead neurons problem with ReLU?
A: Dead neurons = neurons that always output 0 because their weights became negative.

Solution 1: Leaky ReLU
act = nn.LeakyReLU(0.01)  # 0.01 slope for x < 0

Solution 2: PReLU (learned slope)
act = nn.PReLU(num_parameters=64)  # One slope per channel

Q5: How to choose output layer activation?
A:
# Binary classification: Sigmoid
output = torch.sigmoid(logits)

# Multi-class classification: Softmax
output = F.softmax(logits, dim=1)

# Multi-label classification: Sigmoid
output = torch.sigmoid(logits)

# Regression: No activation (or ReLU if output > 0)
output = logits
