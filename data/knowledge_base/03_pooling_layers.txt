POOLING LAYERS
===============

Q1: What is pooling and why use it?
A: Pooling reduces spatial dimensions to:
- Decrease number of parameters and computations
- Create translation invariance
- Expand receptive field
- Reduce overfitting

Q2: What are the different types of pooling?
A:
import torch.nn as nn

# Max Pooling (most common)
maxpool = nn.MaxPool2d(kernel_size=2, stride=2)

# Average Pooling
avgpool = nn.AvgPool2d(kernel_size=2, stride=2)

# Global Average Pooling
global_avgpool = nn.AdaptiveAvgPool2d((1, 1))

# Global Max Pooling
global_maxpool = nn.AdaptiveMaxPool2d((1, 1))

Q3: Difference between Max and Average Pooling?
A:
- Max Pooling: Takes maximum value in region
  → Better for detecting features (presence/absence)
  → More robust to noise
  
- Average Pooling: Computes average of region
  → Smooths features
  → Often used in final layer (Global Average Pooling)

Q4: What is Global Average Pooling and why use it?
A: Global Average Pooling (GAP) computes average of each entire feature map.

Advantages:
- No parameters to learn
- Drastically reduces overfitting
- Links convolutions to classification
- Used in ResNet, Inception, etc.

class CNNWithGAP(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(),
        )
        self.gap = nn.AdaptiveAvgPool2d((1, 1))
        self.classifier = nn.Linear(128, num_classes)
    
    def forward(self, x):
        x = self.features(x)
        x = self.gap(x)
        x = x.view(x.size(0), -1)
        return self.classifier(x)
