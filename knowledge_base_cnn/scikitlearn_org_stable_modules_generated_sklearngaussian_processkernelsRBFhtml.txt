SOURCE_URL: https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RBF.html
SUJET: CNN_DEEP_LEARNING

# RBF#
Radial basis function kernel (aka squared-exponential kernel).
The RBF kernel is a stationary kernel. It is also known as the
“squared exponential” kernel. It is parameterized by a length scale
parameter \(l>0\) , which can either be a scalar (isotropic variant
of the kernel) or a vector with the same number of dimensions as the inputs
X (anisotropic variant of the kernel). The kernel is given by:
where \(l\) is the length scale of the kernel and \(d(\cdot,\cdot)\) is the Euclidean distance.
For advice on how to set the length scale parameter, see e.g. [1] .
This kernel is infinitely differentiable, which implies that GPs with this
kernel as covariance function have mean square derivatives of all orders,
and are thus very smooth.
See [2] , Chapter 4, Section 4.2, for further details of the RBF kernel.
Read more in the User Guide .
Added in version 0.18.
The length scale of the kernel. If a float, an isotropic kernel is
used. If an array, an anisotropic kernel is used where each dimension
of l defines the length-scale of the respective feature dimension.
The lower and upper bound on ‘length_scale’.
If set to “fixed”, ‘length_scale’ cannot be changed during
hyperparameter tuning.
References
David Duvenaud (2014). “The Kernel Cookbook:
Advice on Covariance functions”.
Carl Edward Rasmussen, Christopher K. I. Williams (2006).
“Gaussian Processes for Machine Learning”. The MIT Press.
Examples
```python
>>> from sklearn.datasets import load_iris
>>> from sklearn.gaussian_process import GaussianProcessClassifier
>>> from sklearn.gaussian_process.kernels import RBF
>>> X, y = load_iris(return_X_y=True)
>>> kernel = 1.0 * RBF(1.0)
>>> gpc = GaussianProcessClassifier(kernel=kernel,
...         random_state=0).fit(X, y)
>>> gpc.score(X, y)
0.9866
>>> gpc.predict_proba(X[:2,:])
array([[0.8354, 0.03228, 0.1322],
[0.7906, 0.0652, 0.1441]])
```
Return the kernel k(X, Y) and optionally its gradient.
Left argument of the returned kernel k(X, Y)
Right argument of the returned kernel k(X, Y). If None, k(X, X)
if evaluated instead.
Determines whether the gradient with respect to the log of
the kernel hyperparameter is computed.
Only supported when Y is None.
Kernel k(X, Y)
The gradient of the kernel k(X, X) with respect to the log of the
hyperparameter of the kernel. Only returned when eval_gradient is True.
Returns the log-transformed bounds on the theta.
The log-transformed bounds on the kernel’s hyperparameters theta
Returns a clone of self with given hyperparameters theta.
The hyperparameters
Returns the diagonal of the kernel k(X, X).
The result of this method is identical to np.diag(self(X)); however,
it can be evaluated more efficiently since only the diagonal is
evaluated.
Diagonal of kernel k(X, X)
Get parameters of this kernel.
If True, will return the parameters for this estimator and
contained subobjects that are estimators.
Parameter names mapped to their values.
Returns a list of all hyperparameter specifications.
Returns whether the kernel is stationary.
Returns the number of non-fixed hyperparameters of the kernel.
Returns whether the kernel is defined on fixed-length feature
vectors or generic objects. Defaults to True for backward
compatibility.
Set the parameters of this kernel.
The method works on simple kernels as well as on nested kernels.
The latter have parameters of the form <component>__<parameter> so that it’s possible to update each component of a nested object.
Returns the (flattened, log-transformed) non-fixed hyperparameters.
Note that theta are typically the log-transformed values of the
kernel’s hyperparameters as this representation of the search space
is more amenable for hyperparameter search, as hyperparameters like
length-scales naturally live on a log-scale.
The non-fixed, log-transformed hyperparameters of the kernel
## Gallery examples#
Plot classification probability
Classifier comparison
Comparison of kernel ridge and Gaussian process regression
Probabilistic predictions with Gaussian process classification (GPC)
Gaussian process classification (GPC) on iris dataset
Illustration of Gaussian process classification (GPC) on the XOR dataset
Forecasting of CO2 level on Mona Loa dataset using Gaussian process regression (GPR)
Ability of Gaussian process regression (GPR) to estimate data noise-level
Gaussian Processes regression: basic introductory example
Illustration of prior and posterior Gaussian process for different kernels
### This Page
- Show Source