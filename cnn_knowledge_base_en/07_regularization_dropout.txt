REGULARIZATION AND DROPOUT
===========================

Q1: Why is regularization necessary?
A: Regularization prevents overfitting by limiting model complexity and improving generalization on unseen data.

Q2: What are the main regularization techniques?
A:
1. Dropout: Randomly deactivates neurons
2. L1/L2 Regularization: Penalizes high weights
3. Data Augmentation: Increases data diversity
4. Early Stopping: Stops training before overfitting
5. Batch Normalization: Normalizes activations

Q3: How to implement Dropout?
A:
class CNNWithDropout(nn.Module):
    def __init__(self, num_classes=10, dropout_rate=0.5):
        super().__init__()
        
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            nn.Dropout2d(p=0.1),  # Spatial dropout after conv
            
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            nn.Dropout2d(p=0.2),
        )
        
        self.classifier = nn.Sequential(
            nn.Linear(128 * 8 * 8, 512),
            nn.ReLU(),
            nn.Dropout(p=dropout_rate),  # Classic dropout
            nn.Linear(512, num_classes)
        )

Q4: How to implement L2 regularization?
A:
import torch.optim as optim

# Via optimizer
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)

# L2 regularization = weight decay
# Penalizes large weights during training

Q5: What is DropBlock (better for CNNs)?
A:
class DropBlock2D(nn.Module):
    def __init__(self, drop_prob=0.1, block_size=7):
        super().__init__()
        self.drop_prob = drop_prob
        self.block_size = block_size
    
    def forward(self, x):
        if not self.training or self.drop_prob == 0:
            return x
        
        gamma = self.drop_prob / (self.block_size ** 2)
        mask = torch.bernoulli(torch.ones_like(x) * gamma)
        
        # Expand mask with max pooling
        mask = 1 - nn.functional.max_pool2d(
            mask, 
            kernel_size=self.block_size, 
            stride=1, 
            padding=self.block_size // 2
        )
        
        normalize = mask.numel() / mask.sum()
        return x * mask * normalize

Q6: How to implement early stopping?
A:
class EarlyStopping:
    def __init__(self, patience=7, min_delta=0):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_score = None
        self.early_stop = False
    
    def __call__(self, val_loss, model):
        score = -val_loss
        
        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(model)
        elif score < self.best_score + self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.save_checkpoint(model)
            self.counter = 0
    
    def save_checkpoint(self, model):
        torch.save(model.state_dict(), 'best_model.pth')
