

# tf.VariableStay organized with collectionsSave and categorize content based on your preferences.


tf.Variable(
    initial_value=None,
    trainable=None,
    validate_shape=True,
    caching_device=None,
    name=None,
    variable_def=None,
    dtype=None,
    import_scope=None,
    constraint=None,
    synchronization=tf.VariableSynchronization.AUTO,
    aggregation=tf.compat.v1.VariableAggregation.NONE,
    shape=None,
    experimental_enable_variable_lifting=True
)




### Used in the notebooks
- Introduction to gradients and automatic differentiation
- Custom training loop with Keras and MultiWorkerMirroredStrategy
The Variable() constructor requires an initial value for the variable, which
can be a Tensor of any type and shape. This initial value defines the type
and shape of the variable. After construction, the type and shape of the


v = tf.Variable(1.)
v.assign(2.)
<tf.Variable ... shape=() dtype=float32, numpy=2.0>
v.assign_add(0.5)
<tf.Variable ... shape=() dtype=float32, numpy=2.5>


The shape argument to Variable 's constructor allows you to construct a
variable with a less defined shape than its initial_value :


v = tf.Variable(1., shape=tf.TensorShape(None))
v.assign([[1.]])
<tf.Variable ... shape=<unknown> dtype=float32, numpy=array([[1.]], ...)>


Just like any Tensor , variables created with Variable() can be used as
inputs to operations. Additionally, all the operators overloaded for the Tensor class are carried over to variables.


w = tf.Variable([[1.], [2.]])
x = tf.constant([[3., 4.]])
tf.matmul(w, x)
<tf.Tensor:... shape=(2, 2), ... numpy=
  array([[3., 4.],
         [6., 8.]], dtype=float32)>
tf.sigmoid(w + x)
<tf.Tensor:... shape=(2, 2), ...>


When building a machine learning model it is often convenient to distinguish
between variables holding trainable model parameters and other variables such
as a step variable used to count training steps. To make this easier, the
variable constructor supports a trainable=<bool> parameter. tf.GradientTape watches trainable variables by default:


with tf.GradientTape(persistent=True) as tape:
  trainable = tf.Variable(1.)
  non_trainable = tf.Variable(2., trainable=False)
  x1 = trainable * 2.
  x2 = non_trainable * 3.
tape.gradient(x1, trainable)
<tf.Tensor:... shape=(), dtype=float32, numpy=2.0>
assert tape.gradient(x2, non_trainable) is None  # Unwatched




m = tf.Module()
m.v = tf.Variable([1.])
(<tf.Variable ... shape=(1,) ... numpy=array([1.], dtype=float32)>,)


This tracking then allows saving variable values to training checkpoints , or to SavedModels which include
serialized TensorFlow graphs.


v = tf.Variable(0.)
read_and_decrement = tf.function(lambda: v.assign_sub(0.1))
read_and_decrement()
<tf.Tensor: shape=(), dtype=float32, numpy=-0.1>
read_and_decrement()
<tf.Tensor: shape=(), dtype=float32, numpy=-0.2>




class M(tf.Module):
  def __call__(self, x):
    if not hasattr(self, "v"):  # Or set self.v to None in __init__
      self.v = tf.Variable(x)
    return self.v * x
m = M()
m(2.)
<tf.Tensor: shape=(), dtype=float32, numpy=4.0>
m(3.)
<tf.Tensor: shape=(), dtype=float32, numpy=6.0>
<tf.Variable ... shape=() dtype=float32, numpy=2.0>




## Args
initial_value A Tensor , or Python object convertible to a Tensor ,
a shape specified unless validate_shape is set to False. Can also be a
callable with no argument that returns the initial value when called. In
that case, dtype must be specified. (Note that initializer functions
from init_ops.py must first be bound to a shape before being used here.) trainable If True , GradientTapes automatically watch uses of this
variable. Defaults to True , unless synchronization is set to ON_READ , in which case it defaults to False . validate_shape If False , allows the variable to be initialized with a
value of unknown shape. If True , the default, the shape of initial_value must be known. caching_device Note: This argument is only valid when using a v1-style Session . Optional device string describing where the Variable should
uniquified automatically. variable_def VariableDef protocol buffer. If not None , recreates the
the graph, which must already exist. The graph is not changed. variable_def and the other arguments are mutually exclusive. dtype If set, initial_value will be converted to the given type. If None , either the datatype will be kept (if initial_value is a
Tensor), or convert_to_tensor will decide. import_scope Optional string . Name scope to add to the Variable. Only
after being updated by an Optimizer (e.g. used to implement norm
constraints or value constraints for layer weights). The function must
take as input the unprojected Tensor representing the value of the
variable and return the Tensor for the projected value (which must have
the same shape). Constraints are not safe to use when doing asynchronous
distributed training. synchronization Indicates when a distributed variable will be
aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to
Accepted values are constants defined in the class tf.VariableAggregation . shape (optional) The shape of this variable. If None, the shape of initial_value will be used. When setting this argument to tf.TensorShape(None) (representing an unspecified shape), the variable
can be assigned with values of different shapes. experimental_enable_variable_lifting Whether to lift the variable out if
and you can freely create and use the variable in the tf.function , as if it's a "mutable tf.Tensor ". You can't
return the variable though.


## Raises
ValueError If both variable_def and initial_value are specified. ValueError If the initial value is not specified, or does not have a
shape and validate_shape is True .


## Attributes
constraint Returns the constraint function associated with this variable. device The device of this variable. dtype The DType of this variable. graph The Graph of this variable. initial_value Returns the Tensor used as the initial value for the variable.
Note that this is different from initialized_value() which runs
the op that initializes the variable before returning its value.
This method returns the tensor that is used by the op that initializes
the variable. initializer The initializer operation for this variable. name The name of this variable. op The Operation of this variable. shape The TensorShape of this variable. synchronization


## Child Classes


## Methods


### assign


assign(
    value, use_locking=False, name=None, read_value=True
)


This is essentially a shortcut for assign(self, value) .
value A Tensor . The new value for this variable. use_locking If True , use locking during the assignment. name The name of the operation to be created read_value if True, will return something which evaluates to the new
value of the variable; if False will return the assign op.
Returns The updated variable. If read_value is false, instead returns None in


### assign_add


assign_add(
    delta, use_locking=False, name=None, read_value=True
)


This is essentially a shortcut for assign_add(self, delta) .
delta A Tensor . The value to add to this variable. use_locking If True , use locking during the operation. name The name of the operation to be created read_value if True, will return something which evaluates to the new
value of the variable; if False will return the assign op.


### assign_sub


assign_sub(
    delta, use_locking=False, name=None, read_value=True
)


This is essentially a shortcut for assign_sub(self, delta) .
delta A Tensor . The value to subtract from this variable. use_locking If True , use locking during the operation. name The name of the operation to be created read_value if True, will return something which evaluates to the new
value of the variable; if False will return the assign op.


### batch_scatter_update


batch_scatter_update(
    sparse_delta, use_locking=False, name=None
)


Assigns tf.IndexedSlices to this variable batch-wise.
Analogous to batch_gather . This assumes that this variable and the
num_prefix_dims = sparse_delta.indices.ndims - 1 batch_dim = num_prefix_dims + 1 sparse_delta.updates.shape = sparse_delta.indices.shape + var.shape[
     batch_dim:]
sparse_delta.updates.shape[:num_prefix_dims] == sparse_delta.indices.shape[:num_prefix_dims] == var.shape[:num_prefix_dims]
var[i_1, ..., i_n,
     sparse_delta.indices[i_1, ..., i_n, j]] = sparse_delta.updates[
        i_1, ..., i_n, j]
When sparse_delta.indices is a 1D tensor, this operation is equivalent to scatter_update .
variable and using scatter_update on the subtensors that result of slicing
the first dimension. This is a valid option for ndims = 1 , but less


### count_up_to


count_up_to(
    limit
)


Increments this variable until it reaches limit . (deprecated)
If no error is raised, the Op outputs the value of the variable before
This is essentially a shortcut for count_up_to(self, limit) .
Returns A Tensor that will hold the variable value before the increment. If no


### eval


eval(
    session=None
)


In a session, computes and returns the value of this variable.
This convenience method requires a session where the graph


v = tf.Variable([1, 2])
init = tf.compat.v1.global_variables_initializer()
with tf.compat.v1.Session() as sess:
    sess.run(init)
    # Usage passing the session explicitly.
    print(v.eval(sess))
    # Usage with the default session.  The 'with' block
    # above makes 'sess' the default session.
    print(v.eval())


Returns A numpy ndarray with a copy of the value of this variable.


### experimental_ref


### from_proto


from_proto(
    variable_def, import_scope=None
)


Returns a Variable object created from variable_def .


### gather_nd


gather_nd(
    indices, name=None
)


Gather slices from params into a Tensor with shape specified by indices .
indices A Tensor . Must be one of the following types: int32 , int64 .
Index tensor. name A name for the operation (optional).
Returns A Tensor . Has the same type as params .


### get_shape


get_shape() -> tf.TensorShape




### initialized_value
Returns the value of the initialized variable. (deprecated)


# Initialize 'v' with a random tensor.
v = tf.Variable(tf.random.truncated_normal([10, 40]))
# Use `initialized_value` to guarantee that `v` has been
# initialized before its value is used to initialize `w`.
# The random values are picked only once.
w = tf.Variable(v.initialized_value() * 2.0)


Returns A Tensor holding the value of this variable after its initializer


### load


load(
    value, session=None
)


Load new value into this variable. (deprecated)


v = tf.Variable([1, 2])
init = tf.compat.v1.global_variables_initializer()
with tf.compat.v1.Session() as sess:
    sess.run(init)
    # Usage passing the session explicitly.
    v.load([2, 3], sess)
    print(v.eval(sess)) # prints [2 3]
    # Usage with the default session.  The 'with' block
    # above makes 'sess' the default session.
    v.load([3, 4], sess)
    print(v.eval()) # prints [3 4]




### read_value
Can be different from value() if it's on another device, with control
Returns A Tensor containing the value of the variable.


### ref
We can't put variables in a set/dictionary as variable.__hash__() is no
longer available starting Tensorflow 2.0.


x = tf.Variable(5)
y = tf.Variable(10)
z = tf.Variable(10)
variable_set = {x, y, z}
Traceback (most recent call last):
TypeError: Variable is unhashable. Instead, use tensor.ref() as the key.
variable_dict = {x: 'five', y: 'ten'}
Traceback (most recent call last):
TypeError: Variable is unhashable. Instead, use tensor.ref() as the key.


Instead, we can use variable.ref() .


variable_set = {x.ref(), y.ref(), z.ref()}
x.ref() in variable_set
variable_dict = {x.ref(): 'five', y.ref(): 'ten', z.ref(): 'ten'}
variable_dict[y.ref()]


Also, the reference object provides .deref() function that returns the


x = tf.Variable(5)
x.ref().deref()
<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=5>




### scatter_add


scatter_add(
    sparse_delta, use_locking=False, name=None
)




### scatter_div


scatter_div(
    sparse_delta, use_locking=False, name=None
)




### scatter_max


scatter_max(
    sparse_delta, use_locking=False, name=None
)




### scatter_min


scatter_min(
    sparse_delta, use_locking=False, name=None
)




### scatter_mul


scatter_mul(
    sparse_delta, use_locking=False, name=None
)




### scatter_nd_add


scatter_nd_add(
    indices, updates, name=None
)


The Variable has rank P and indices is a Tensor of rank Q .
indices must be integer tensor, containing indices into self.
It must be shape [d_0, ..., d_{Q-2}, K] where 0 < K <= P .
The innermost dimension of indices (with length K ) corresponds to
indices into elements (if K = P ) or slices (if K < P ) along the K th
updates is Tensor of rank Q-1+P-K with shape:


[d_0, ..., d_{Q-2}, self.shape[K], ..., self.shape[P-1]].


For example, say we want to add 4 scattered elements to a rank-1 tensor to


v = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])
    indices = tf.constant([[4], [3], [1] ,[7]])
    updates = tf.constant([9, 10, 11, 12])
    v.scatter_nd_add(indices, updates)
    print(v)




[1, 13, 3, 14, 14, 6, 7, 20]




### scatter_nd_sub


scatter_nd_sub(
    indices, updates, name=None
)


Assuming the variable has rank P and indices is a Tensor of rank Q .


v = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])
    indices = tf.constant([[4], [3], [1] ,[7]])
    updates = tf.constant([9, 10, 11, 12])
    v.scatter_nd_sub(indices, updates)
    print(v)




[1, -9, 3, -6, -4, 6, 7, -4]




### scatter_nd_update


scatter_nd_update(
    indices, updates, name=None
)




v = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])
    indices = tf.constant([[4], [3], [1] ,[7]])
    updates = tf.constant([9, 10, 11, 12])
    v.scatter_nd_update(indices, updates)
    print(v)




[1, 11, 3, 10, 9, 6, 7, 12]




### scatter_sub


scatter_sub(
    sparse_delta, use_locking=False, name=None
)




### scatter_update


scatter_update(
    sparse_delta, use_locking=False, name=None
)




### set_shape


set_shape(
    shape
)


Overrides the shape for this variable.
shape the TensorShape representing the overridden shape.


### sparse_read


sparse_read(
    indices, name=None
)


indices The index Tensor .  Must be one of the following types: int32 , int64 . Must be in range [0, params.shape[axis]) . name A name for the operation (optional).


### to_proto


to_proto(
    export_scope=None
)


Converts a Variable to a VariableDef protocol buffer.


### value
of the variable call it automatically through a convert_to_tensor() call.
Returns a Tensor which holds the value of the variable.  You can not
assign a new value to this tensor as it is not a reference to the variable.
To avoid copies, if the consumer of the returned value is on the same device
as the variable, this actually returns the live value of the variable, not


### __abs__


__abs__(
    name=None
)




### __add__


### __and__


### __div__


### __eq__


### __floordiv__


__floordiv__(
    y
)




### __ge__


__ge__(
    y: Annotated[Any, tf.raw_ops.Any],
    name=None
) -> Annotated[Any, tf.raw_ops.Any]


Returns the truth value of (x >= y) element-wise.


x = tf.constant([5, 4, 6, 7])
y = tf.constant([5, 2, 5, 10])
tf.math.greater_equal(x, y) ==> [True, True, True, False]
x = tf.constant([5, 4, 6, 7])
y = tf.constant([5])
tf.math.greater_equal(x, y) ==> [True, False, True, True]


x A Tensor . Must be one of the following types: float32 , float64 , int32 , uint8 , int16 , int8 , int64 , bfloat16 , uint16 , half , uint32 , uint64 . y A Tensor . Must have the same type as x . name A name for the operation (optional).
Returns A Tensor of type bool .


### __getitem__


__getitem__(
    slice_spec
)


This allows creating a sub-tensor from part of the current contents
of a variable. See tf.Tensor. getitem for detailed examples
operation for grouping or passing to sess.run() in TF1.


import tensorflow as tf
A = tf.Variable([[1,2,3], [4,5,6], [7,8,9]], dtype=tf.float32)
print(A[:2, :2])  # => [[1,2], [4,5]]
A[:2,:2].assign(22. * tf.ones((2, 2))))
print(A) # => [[22, 22, 3], [22, 22, 6], [7,8,9]]


Note that assignments currently do not support NumPy broadcasting
var An ops.Variable object. slice_spec The arguments to Tensor. getitem .
Returns The appropriate slice of "tensor", based on "slice_spec".
As an operator. The operator also has a assign() method
ellipsis, tf.newaxis or int32/int64 tensors.


### __gt__


__gt__(
    y: Annotated[Any, tf.raw_ops.Any],
    name=None
) -> Annotated[Any, tf.raw_ops.Any]


Returns the truth value of (x > y) element-wise.


x = tf.constant([5, 4, 6])
y = tf.constant([5, 2, 5])
tf.math.greater(x, y) ==> [False, True, True]
x = tf.constant([5, 4, 6])
y = tf.constant([5])
tf.math.greater(x, y) ==> [False, False, True]




### __invert__


__invert__(
    name=None
)




### __iter__


### __le__


__le__(
    y: Annotated[Any, tf.raw_ops.Any],
    name=None
) -> Annotated[Any, tf.raw_ops.Any]


Returns the truth value of (x <= y) element-wise.


x = tf.constant([5, 4, 6])
y = tf.constant([5])
tf.math.less_equal(x, y) ==> [True, True, False]
x = tf.constant([5, 4, 6])
y = tf.constant([5, 6, 6])
tf.math.less_equal(x, y) ==> [True, True, True]




### __lt__


__lt__(
    y: Annotated[Any, tf.raw_ops.Any],
    name=None
) -> Annotated[Any, tf.raw_ops.Any]


Returns the truth value of (x < y) element-wise.


x = tf.constant([5, 4, 6])
y = tf.constant([5])
tf.math.less(x, y) ==> [False, True, False]
x = tf.constant([5, 4, 6])
y = tf.constant([5, 6, 7])
tf.math.less(x, y) ==> [False, True, True]




### __matmul__


### __mod__


### __mul__


### __ne__


### __neg__


__neg__(
    name=None
) -> Annotated[Any, tf.raw_ops.Any]


x A Tensor . Must be one of the following types: bfloat16 , half , float32 , float64 , int8 , int16 , int32 , int64 , complex64 , complex128 . name A name for the operation (optional).
Returns A Tensor . Has the same type as x .


### __or__


### __pow__


### __radd__


### __rand__


### __rdiv__


### __rfloordiv__


__rfloordiv__(
    x
)




### __rmatmul__


### __rmod__


### __rmul__


### __ror__


### __rpow__


### __rsub__


### __rtruediv__


__rtruediv__(
    x
)




### __rxor__


### __sub__


### __truediv__


### __xor__
Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Some content is licensed under the numpy license .

# Source: https://www.tensorflow.org/api_docs/python/tf/Variable