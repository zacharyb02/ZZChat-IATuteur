

# tf.GradientTapeStay organized with collectionsSave and categorize content based on your preferences.
Main aliases tf.autodiff.GradientTape Compat aliases for migration See Migration guide for
more details. tf.compat.v1.GradientTape


tf.GradientTape(
    persistent=False, watch_accessed_variables=True
)




### Used in the notebooks
- Introduction to gradients and automatic differentiation
- Deep Convolutional Generative Adversarial Network
at least one of their inputs is being "watched".
Trainable variables (created by tf.Variable or tf.compat.v1.get_variable ,
where trainable=True is default in both cases) are automatically watched.
Tensors can be manually watched by invoking the watch method on this context
For example, consider the function y = x * x . The gradient at x = 3.0 can


x = tf.constant(3.0)
with tf.GradientTape() as g:
  g.watch(x)
  y = x * x
dy_dx = g.gradient(y, x)
print(dy_dx)
tf.Tensor(6.0, shape=(), dtype=float32)


GradientTapes can be nested to compute higher-order derivatives. For example,


x = tf.constant(5.0)
with tf.GradientTape() as g:
  g.watch(x)
  with tf.GradientTape() as gg:
    gg.watch(x)
    y = x * x
  dy_dx = gg.gradient(y, x)  # dy_dx = 2 * x
d2y_dx2 = g.gradient(dy_dx, x)  # d2y_dx2 = 2
print(dy_dx)
tf.Tensor(10.0, shape=(), dtype=float32)
print(d2y_dx2)
tf.Tensor(2.0, shape=(), dtype=float32)


By default, the resources held by a GradientTape are released as soon as
GradientTape.gradient() method is called. To compute multiple gradients over
the same computation, create a persistent gradient tape. This allows multiple
calls to the gradient() method as resources are released when the tape object


x = tf.constant(3.0)
with tf.GradientTape(persistent=True) as g:
  g.watch(x)
  y = x * x
  z = y * y
dz_dx = g.gradient(z, x)  # (4*x^3 at x = 3)
print(dz_dx)
tf.Tensor(108.0, shape=(), dtype=float32)
dy_dx = g.gradient(y, x)
print(dy_dx)
tf.Tensor(6.0, shape=(), dtype=float32)


By default GradientTape will automatically watch any trainable variables that
variables are watched you can disable automatic tracking by passing watch_accessed_variables=False to the tape constructor:


x = tf.Variable(2.0)
w = tf.Variable(5.0)
with tf.GradientTape(
    watch_accessed_variables=False, persistent=True) as tape:
  tape.watch(x)
  y = x ** 2  # Gradients will be available for `x`.
  z = w ** 3  # No gradients will be available as `w` isn't being watched.
dy_dx = tape.gradient(y, x)
print(dy_dx)
tf.Tensor(4.0, shape=(), dtype=float32)
# No gradients will be available as `w` isn't being watched.
dz_dw = tape.gradient(z, w)
print(dz_dw)


Note that when using models you should ensure that your variables exist when
using watch_accessed_variables=False . Otherwise it's quite easy to make your
first iteration not have any gradients:


a = tf.keras.layers.Dense(32)
b = tf.keras.layers.Dense(32)
with tf.GradientTape(watch_accessed_variables=False) as tape:
  tape.watch(a.variables)  # Since `a.build` has not been called at this point
                           # `a.variables` will return an empty list and the
                           # tape will not be watching anything.
  result = b(a(inputs))
  tape.gradient(result, a.variables)  # The result of this computation will be
                                      # a list of `None`s since a's variables
                                      # are not being watched.


Note that only tensors with real or complex dtypes are differentiable.


## Args
persistent Boolean controlling whether a persistent gradient tape
be made to the gradient() method on this object. watch_accessed_variables Boolean controlling whether the tape will
automatically watch any (trainable) variables accessed while the tape
is active. Defaults to True meaning gradients can be requested from any
request gradients from.


## Methods


### batch_jacobian


batch_jacobian(
    target,
    unconnected_gradients=tf.UnconnectedGradients.NONE,
    parallel_iterations=None,
    experimental_use_pfor=True
)


tf.stack([self.jacobian(y[i], x[i]) for i in range(x.shape[0])]) .
Note that compared to GradientTape.jacobian which computes gradient of
each output value w.r.t each input value, this function is useful when target[i,...] is independent of source[j,...] for j != i . This
assumption allows more efficient computation as compared to GradientTape.jacobian . The output, as well as intermediate activations,


with tf.GradientTape() as g:
  x = tf.constant([[1., 2.], [3., 4.]], dtype=tf.float32)
  g.watch(x)
  y = x * x
batch_jacobian = g.batch_jacobian(y, x)
# batch_jacobian is [[[2,  0], [0,  4]], [[6,  0], [0,  8]]]


target A tensor with rank 2 or higher and with shape [b, y1, ..., y_n]. target[i,...] should only depend on source[i,...] . source A tensor with rank 2 or higher and with shape [b, x1, ..., x_m]. unconnected_gradients a value which can either hold 'none' or 'zero' and
alters the value which will be returned if the target and sources are
'UnconnectedGradients' and it defaults to 'none'. parallel_iterations A knob to control how many iterations are dispatched
Returns A tensor t with shape [b, y_1, ..., y_n, x1, ..., x_m] where t[i, ...] is the jacobian of target[i, ...] w.r.t. source[i, ...] , i.e. stacked


### gradient


gradient(
    target,
    output_gradients=None,
    unconnected_gradients=tf.UnconnectedGradients.NONE
)


Computes the gradient using operations recorded in context of this tape.
In addition to Tensors, gradient also supports RaggedTensors. For example,


x = tf.ragged.constant([[1.0, 2.0], [3.0]])
with tf.GradientTape() as g:
  g.watch(x)
  y = x * x
g.gradient(y, x)
<tf.RaggedTensor [[2.0, 4.0], [6.0]]>


target a list or nested structure of Tensors or Variables or
CompositeTensors to be differentiated. sources a list or nested structure of Tensors or Variables or
CompositeTensors. target will be differentiated against elements in sources . output_gradients a list of gradients, one for each differentiable
element of target. Defaults to None. unconnected_gradients a value which can either hold 'none' or 'zero' and
alters the value which will be returned if the target and sources are
'UnconnectedGradients' and it defaults to 'none'.
Returns a list or nested structure of Tensors (or IndexedSlices, or None, or
CompositeTensor), one for each element in sources . Returned structure
RuntimeError If called on a used, non-persistent tape. RuntimeError If called inside the context of the tape. TypeError If the target is a None object. ValueError If the target is a variable or if unconnected gradients is


### jacobian


jacobian(
    target,
    unconnected_gradients=tf.UnconnectedGradients.NONE,
    parallel_iterations=None,
    experimental_use_pfor=True
)




with tf.GradientTape() as g:
  x  = tf.constant([1.0, 2.0])
  g.watch(x)
  y = x * x
jacobian = g.jacobian(y, x)
# jacobian value is [[2., 0.], [0., 4.]]


target Tensor to be differentiated. sources a list or nested structure of Tensors or Variables. target will be differentiated against elements in sources . unconnected_gradients a value which can either hold 'none' or 'zero' and
alters the value which will be returned if the target and sources are
'UnconnectedGradients' and it defaults to 'none'. parallel_iterations A knob to control how many iterations are dispatched
Returns A list or nested structure of Tensors (or None), one for each element in sources . Returned structure is the same as the structure of sources .
Note if any gradient is sparse (IndexedSlices), jacobian function
currently makes it dense and returns a Tensor instead. This may change in


### reset


with tf.GradientTape() as t:
  loss = loss_fn()
with tf.GradientTape() as t:
  loss += other_loss_fn()
t.gradient(loss, ...)  # Only differentiates other_loss_fn, not loss_fn
# The following is equivalent to the above
with tf.GradientTape() as t:
  loss = loss_fn()
  t.reset()
  loss += other_loss_fn()
t.gradient(loss, ...)  # Only differentiates other_loss_fn, not loss_fn




with tf.GradientTape() as t:
  loss = ...
  if loss > k:
    t.reset()




### stop_recording


stop_recording()




x = tf.constant(4.0)
with tf.GradientTape() as tape:
  with tape.stop_recording():
    y = x ** 2
dy_dx = tape.gradient(y, x)
print(dy_dx)




### watch
Ensures that tensor is being traced by this tape.
tensor a Tensor/Variable or list of Tensors/Variables.
ValueError if it encounters something that is not a tensor.


### watched_variables


### __enter__


### __exit__


__exit__(
    typ, value, traceback
)


Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Some content is licensed under the numpy license .

# Source: https://www.tensorflow.org/api_docs/python/tf/GradientTape