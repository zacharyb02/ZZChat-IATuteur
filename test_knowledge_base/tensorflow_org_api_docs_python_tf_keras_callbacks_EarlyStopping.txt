

# tf.keras.callbacks.EarlyStoppingStay organized with collectionsSave and categorize content based on your preferences.
Stop training when a monitored metric has stopped improving.


tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    min_delta=0,
    patience=0,
    verbose=0,
    mode='auto',
    baseline=None,
    restore_best_weights=False,
    start_from_epoch=0
)




### Used in the notebooks
- Transfer learning with YAMNet for environmental sound classification
- Introduction to the Keras Tuner
Assuming the goal of a training is to minimize the loss. With this, the
metric to be monitored would be 'loss' , and mode would be 'min' . A model.fit() training loop will check at end of every epoch whether
the loss is no longer decreasing, considering the min_delta and patience if applicable. Once it's found no longer decreasing, model.stop_training is marked True and the training terminates.
To make it so, pass the loss or metrics at model.compile() .


## Args
monitor Quantity to be monitored. Defaults to "val_loss" . min_delta Minimum change in the monitored quantity to qualify as an
count as no improvement. Defaults to 0 . patience Number of epochs with no improvement after which training will
messages when the callback takes an action. Defaults to 0 . mode One of {"auto", "min", "max"} . In min mode, training will stop
training will stop if the model doesn't show improvement over the
baseline. Defaults to None . restore_best_weights Whether to restore model weights from the epoch
with the best value of the monitored quantity. If False , the model
weights obtained at the last step of training are used. An epoch
will be restored regardless of the performance relative to the baseline . If no epoch improves on baseline , training will run
for patience epochs and restore weights from the best epoch in
that set. Defaults to False . start_from_epoch Number of epochs to wait before starting to monitor
improvement is expected and thus training will not be stopped.


callback = keras.callbacks.EarlyStopping(monitor='loss',
                                              patience=3)
# This callback will stop the training when there is no improvement in
# the loss for three consecutive epochs.
model = keras.models.Sequential([keras.layers.Dense(10)])
model.compile(keras.optimizers.SGD(), loss='mse')
history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),
                    epochs=10, batch_size=1, callbacks=[callback],
                    verbose=0)
len(history.history['loss'])  # Only 4 epochs are run.




## Attributes


## Methods


### get_monitor_value


get_monitor_value(
    logs
)




### on_batch_begin


on_batch_begin(
    batch, logs=None
)


A backwards compatibility alias for on_train_batch_begin .


### on_batch_end


on_batch_end(
    batch, logs=None
)


A backwards compatibility alias for on_train_batch_end .


### on_epoch_begin


on_epoch_begin(
    epoch, logs=None
)


Called at the start of an epoch.
epoch Integer, index of epoch. logs Dict. Currently no data is passed to this argument for this


### on_epoch_end


on_epoch_end(
    epoch, logs=None
)


epoch Integer, index of epoch. logs Dict, metric results for this training epoch, and for the
validation epoch if validation is performed. Validation result
keys are prefixed with val_ . For training epoch, the values of
the Model 's metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} .


### on_predict_batch_begin


on_predict_batch_begin(
    batch, logs=None
)


Called at the beginning of a batch in predict methods.
Note that if the steps_per_execution argument to compile in Model is set to N , this method will only be called every N batches.
batch Integer, index of batch within the current epoch. logs Dict. Currently no data is passed to this argument for this


### on_predict_batch_end


on_predict_batch_end(
    batch, logs=None
)


Called at the end of a batch in predict methods.
batch Integer, index of batch within the current epoch. logs Dict. Aggregated metric results up until this batch.


### on_predict_begin


on_predict_begin(
    logs=None
)


Called at the beginning of prediction.


### on_predict_end


on_predict_end(
    logs=None
)


Called at the end of prediction.


### on_test_batch_begin


on_test_batch_begin(
    batch, logs=None
)


Called at the beginning of a batch in evaluate methods.
Also called at the beginning of a validation batch in the fit methods, if validation data is provided.


### on_test_batch_end


on_test_batch_end(
    batch, logs=None
)


Called at the end of a batch in evaluate methods.
Also called at the end of a validation batch in the fit methods, if validation data is provided.


### on_test_begin


on_test_begin(
    logs=None
)


Called at the beginning of evaluation or validation.


### on_test_end


on_test_end(
    logs=None
)


Called at the end of evaluation or validation.
logs Dict. Currently the output of the last call to on_test_batch_end() is passed to this argument for this method


### on_train_batch_begin


on_train_batch_begin(
    batch, logs=None
)


Called at the beginning of a training batch in fit methods.


### on_train_batch_end


on_train_batch_end(
    batch, logs=None
)


Called at the end of a training batch in fit methods.


### on_train_begin


on_train_begin(
    logs=None
)


Called at the beginning of training.


### on_train_end


on_train_end(
    logs=None
)


logs Dict. Currently the output of the last call to on_epoch_end() is passed to this argument for this method but


### set_model


set_model(
    model
)




### set_params


set_params(
    params
)


Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Some content is licensed under the numpy license .

# Source: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping