

# tf.train.CheckpointStay organized with collectionsSave and categorize content based on your preferences.


tf.train.Checkpoint(
    root=None, **kwargs
)




### Used in the notebooks
- Introduction to modules, layers, and models
- Custom training with tf.distribute.Strategy
- Distributed training with DTensors
- Custom training loop with Keras and MultiWorkerMirroredStrategy
- Multi-worker training with Keras
TensorFlow objects may contain trackable state, such as tf.Variable s, tf.keras.optimizers.Optimizer implementations, tf.data.Dataset iterators, tf.keras.Layer implementations, or tf.keras.Model implementations.


model = tf.keras.Model(...)
checkpoint = tf.train.Checkpoint(model)
# Save a checkpoint to /tmp/training_checkpoints-{save_counter}. Every time
# checkpoint.save is called, the save counter is increased.
save_path = checkpoint.save('/tmp/training_checkpoints')
# Restore the checkpointed values to the `model` object.
checkpoint.restore(save_path)




import tensorflow as tf
import os
checkpoint_directory = "/tmp/training_checkpoints"
checkpoint_prefix = os.path.join(checkpoint_directory, "ckpt")
# Create a Checkpoint that will manage two objects with trackable state,
# one we name "optimizer" and the other we name "model".
checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)
status = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_directory))
for _ in range(num_training_steps):
  optimizer.minimize( ... )  # Variables will be restored on creation.
status.assert_consumed()  # Optional sanity checks.
checkpoint.save(file_prefix=checkpoint_prefix)


Checkpoint.save() and Checkpoint.restore() write and read object-based
checkpoints, in contrast to TensorFlow 1.x's tf.compat.v1.train.Saver which
graph of dependencies between Python objects ( Layer s, Optimizer s, Variable s, etc.) with named edges, and this graph is used to match variables
TensorFlow classes like Layer s and Optimizer s will automatically add
dependencies on their own variables (e.g. "kernel" and "bias" for tf.keras.layers.Dense ). Inheriting from tf.keras.Model makes managing
dependencies easy in user-defined classes, since Model hooks into attribute


class Regress(tf.keras.Model):
  def __init__(self):
    super().__init__()
    self.input_transform = tf.keras.layers.Dense(10)
    # ...
  def call(self, inputs):
    x = self.input_transform(inputs)
    # ...


This Model has a dependency named "input_transform" on its Dense layer,
by the Dense layer.
This function differs slightly from the Keras Model save_weights function. tf.keras.Model.save_weights creates a checkpoint file with the name
using filepath as the prefix for the checkpoint file names. Aside from this, model.save_weights() and tf.train.Checkpoint(model).save() are equivalent.
See the guide to training


## Args
nested structure of trackable objects ( list , dict , or tuple ).


## Raises
objects from the ones listed in attributes in kwargs (e.g. root.child = A and tf.train.Checkpoint(root, child=B) are
incompatible).


## Attributes
save_counter Incremented when save() is called. Used to number


## Methods


### read


read(
    save_path, options=None
)


Reads a training checkpoint written with write .
This method is just like restore() but does not expect the save_counter variable in the checkpoint. It only restores the objects that the checkpoint
management utilities that use write() instead of save() and have their


# Create a checkpoint with write()
ckpt = tf.train.Checkpoint(v=tf.Variable(1.))
path = ckpt.write('/tmp/my_checkpoint')
# Later, load the checkpoint with read()
# With restore() assert_consumed() would have failed.
checkpoint.read(path).assert_consumed()
# You can also pass options to read(). For example this
# runs the IO ops on the localhost:
options = tf.train.CheckpointOptions(
    experimental_io_device="/job:localhost")
checkpoint.read(path, options=options)


save_path The path to the checkpoint as returned by write . options Optional tf.train.CheckpointOptions object.


### restore


restore(
    save_path, options=None
)


Restores a training checkpoint.
This method is intended to be used to load checkpoints created by save() .
For checkpoints created by write() use the read() method which does not
expect the save_counter variable added by save() .
restore() either assigns values immediately if variables to restore have
corresponding object in the checkpoint (the restore request will queue in
any trackable object waiting for the expected dependency to be added).


checkpoint = tf.train.Checkpoint( ... )
checkpoint.restore(path)
# You can additionally pass options to restore():
options = tf.CheckpointOptions(experimental_io_device="/job:localhost")
checkpoint.restore(path, options=options)


take place, use the assert_consumed() method of the status object returned
by restore() :


checkpoint.restore(path, options=options).assert_consumed()


Name-based tf.compat.v1.train.Saver checkpoints from TensorFlow 1.x can be
Loading from SavedModel checkpoints
To load values from a SavedModel, just pass the SavedModel directory


model = tf.keras.Model(...)
tf.saved_model.save(model, path)  # or model.save(path, save_format='tf')
checkpoint = tf.train.Checkpoint(model)
checkpoint.restore(path).expect_partial()


This example calls expect_partial() on the loaded status, since
SavedModels saved from Keras often generates extra keys in the checkpoint.
save_path The path to the checkpoint, as returned by save or tf.train.latest_checkpoint . If the checkpoint was written by the
variables. This path may also be a SavedModel directory. options Optional tf.train.CheckpointOptions object.
The returned status object has the following methods:
- assert_consumed() :
checkpoint. This method returns the status object, and so may be
- assert_existing_objects_matched() :
objects. For example a tf.keras.Layer object which has not yet been
checkpoint into a new Python program, e.g. a training checkpoint with
a tf.compat.v1.train.Optimizer was saved but only the state required
inference is being loaded. This method returns the status object, and
- assert_nontrivial_match() : Asserts that something aside from the root
- expect_partial() : Silence warnings about incomplete checkpoint
(often at program shutdown).
NotFoundError if the a checkpoint or SavedModel cannot be found at save_path .


### save


save(
    file_prefix, options=None
)


Saves a training checkpoint and provides basic checkpoint management.
trackable objects it depends on at the time Checkpoint.save() is
save is a basic convenience wrapper around the write method,
metadata used by tf.train.latest_checkpoint . More advanced checkpoint
( tf.train.CheckpointManager for example).


step = tf.Variable(0, name="step")
checkpoint = tf.train.Checkpoint(step=step)
checkpoint.save("/tmp/ckpt")
# Later, read the checkpoint with restore()
checkpoint.restore("/tmp/ckpt-1")
# You can also pass options to save() and restore(). For example this
# runs the IO ops on the localhost:
options = tf.train.CheckpointOptions(experimental_io_device="/job:localhost")
checkpoint.save("/tmp/ckpt", options=options)
# Later, read the checkpoint with restore()
checkpoint.restore("/tmp/ckpt-1", options=options)


(/path/to/directory/and_a_prefix). Names are generated based on this


### sync


### write


write(
    file_prefix, options=None
)


trackable objects it depends on at the time Checkpoint.write() is
metadata used by tf.train.latest_checkpoint . It is primarily intended for


step = tf.Variable(0, name="step")
checkpoint = tf.Checkpoint(step=step)
checkpoint.write("/tmp/ckpt")
# Later, read the checkpoint with read()
checkpoint.read("/tmp/ckpt")
# You can also pass options to write() and read(). For example this
# runs the IO ops on the localhost:
options = tf.CheckpointOptions(experimental_io_device="/job:localhost")
checkpoint.write("/tmp/ckpt", options=options)
# Later, read the checkpoint with read()
checkpoint.read("/tmp/ckpt", options=options)


(/path/to/directory/and_a_prefix). options Optional tf.train.CheckpointOptions object.
Returns The full path to the checkpoint (i.e. file_prefix ).
Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Some content is licensed under the numpy license .

# Source: https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint